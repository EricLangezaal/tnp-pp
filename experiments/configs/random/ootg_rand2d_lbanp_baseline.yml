# Latent Bottlenecked Attention Neural Process
# only hparam we need to choose is num_latents!

generators:
  train:
    _target_: tnp.data.on_off_grid.RandomOOTGGenerator
    num_off_grid_context: ${params.num_off_the_grid_context}
    grid_shape: ${params.grid_shape}
    num_targets: ${params.num_targets}
    dim: ${params.x_dim}
    samples_per_epoch: 8000
    batch_size: 16
  val:
    _target_: tnp.data.on_off_grid.RandomOOTGGenerator
    num_off_grid_context: ${params.num_off_the_grid_context}
    grid_shape: ${params.grid_shape}
    num_targets: ${params.num_targets}
    dim: ${params.x_dim}
    samples_per_epoch: 2000
    batch_size: 16

model:
  _target_: tnp.models.ootg_tnp.OOTG_TNPD
  encoder: ${lbanp_encoder}
  decoder: ${lbanp_decoder}
  likelihood: ${likelihood}

lbanp_encoder:
  _target_: tnp.models.lbanp.OOTGNestedLBANPEncoder
  perceiver_encoder: ${perceiver_encoder}
  xy_encoder: ${xy_encoder}
  x_encoder: ${siren_net}

perceiver_encoder:
  _target_: tnp.networks.transformer.NestedPerceiverEncoder
  num_latents: ${params.num_latents}
  mhsa_layer: ${mhsa_layer}
  mhca_ctoq_layer: ${mhca_ctoq_layer}
  mhca_qtot_layer: ${mhca_qtot_layer}
  num_layers: ${params.num_layers}

mhsa_layer:
  _target_: tnp.networks.attention_layers.MultiHeadSelfAttentionLayer
  embed_dim: ${params.embed_dim}
  num_heads: ${params.num_heads}
  head_dim: ${params.head_dim}
  feedforward_dim: ${params.embed_dim}
  norm_first: ${params.norm_first}

mhca_ctoq_layer:
  _target_: tnp.networks.attention_layers.MultiHeadCrossAttentionLayer
  embed_dim: ${params.embed_dim}
  num_heads: ${params.num_heads}
  head_dim: ${params.head_dim}
  feedforward_dim: ${params.embed_dim}
  norm_first: ${params.norm_first}

mhca_qtot_layer:
  _target_: tnp.networks.attention_layers.MultiHeadCrossAttentionLayer
  embed_dim: ${params.embed_dim}
  num_heads: ${params.num_heads}
  head_dim: ${params.head_dim}
  feedforward_dim: ${params.embed_dim}
  norm_first: ${params.norm_first}

xy_encoder:
  _target_: tnp.networks.mlp.MLP
  # 2 from preprocessing y, ydim, and output of sirennet
  in_dim: ${eval:'2 + ${params.y_dim} + ${params.num_legendre_polys} ** 2 + ${params.x_dim} - 2'} 
  out_dim: ${params.embed_dim}
  num_layers: 2
  width: ${params.embed_dim}

siren_net:
  _target_: tnp.networks.mlp.MLPWithEmbedding
  embedding: ${spherical_harmonics}
  in_dim: ${eval:'${params.num_legendre_polys} ** 2'}
  out_dim: ${eval:'${params.num_legendre_polys} ** 2'}
  num_layers: 2
  width: ${eval:'${params.num_legendre_polys} ** 2'}
  ignore_dims: ${params.siren_net_ignore_dims}

spherical_harmonics:
  _target_: tnp.networks.embeddings.SphericalHarmonicsEmbedding
  num_legendre_polys: ${params.num_legendre_polys}

lbanp_decoder:
  _target_: tnp.models.lbanp.NestedLBANPDecoder
  z_decoder: ${z_decoder}

z_decoder:
  _target_: tnp.networks.mlp.MLP
  in_dim: ${params.embed_dim}
  out_dim: ${eval:'2 * ${params.dim_y}'}
  num_layers: 2
  width: ${params.embed_dim}

likelihood:
  _target_: tnp.likelihoods.gaussian.HeteroscedasticNormalLikelihood

optimiser:
  _target_: torch.optim.AdamW
  _partial_: True
  lr: 5.0e-4

params:
  epochs: 2

  x_dim: 2
  y_dim: 1
  
  grid_shape: [120, 240]
  num_off_the_grid_context: 1250
  num_targets: 500

  embed_dim: 128
  num_heads: 8
  head_dim: 16
  norm_first: True
  num_layers: 5

  num_latents: 16

  siren_net_ignore_dims: null
  num_legendre_polys: 10


misc:
  logging: False
  gradient_clip_val: 0.5
  resume_from_checkpoint: null
