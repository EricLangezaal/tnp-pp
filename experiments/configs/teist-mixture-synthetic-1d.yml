generators:
  train:
    _target_: icicl.data.synthetic.SyntheticGeneratorMixture
    generators:
      - ${generators.uni_generator}
      - ${generators.bi_generator}
    mixture_probs: [0.5, 0.5]
    dim: ${params.dim_x}
    min_num_ctx: 1
    max_num_ctx: 64
    min_num_trg: 128
    max_num_trg: 128
    samples_per_epoch: 16000
    batch_size: 16
  val:
    _target_: icicl.data.synthetic.SyntheticGeneratorMixture
    generators:
      - ${generators.uni_generator}
      - ${generators.bi_generator}
    mixture_probs: [0.5, 0.5]
    dim: ${params.dim_x}
    min_num_ctx: 1
    max_num_ctx: 64
    min_num_trg: 128
    max_num_trg: 128
    samples_per_epoch: 4096
    batch_size: 16
  uni_generator:
    _target_: icicl.data.gp.RandomScaleGPGenerator
    dim: ${params.dim_x}
    kernel_type: eq
    min_log10_lengthscale: -0.602
    max_log10_lengthscale: 0.602
    noise_std: 0.2
    min_num_ctx: 1
    max_num_ctx: 64
    min_num_trg: 128
    max_num_trg: 128
    context_range: [[-2.0, 2.0]]
    target_range: [[-3.0, 3.0]]
    samples_per_epoch: 16384
    batch_size: 16
  bi_generator:
    _target_: icicl.data.gp.RandomScaleGPGeneratorBimodalInput
    dim: ${params.dim_x}
    kernel_type: eq
    min_log10_lengthscale: -0.602
    max_log10_lengthscale: 0.602
    noise_std: 0.2
    min_num_ctx: 1
    max_num_ctx: 64
    min_num_trg: 128
    max_num_trg: 128
    context_range: [[[-4.0, -1.0], [1.0, 4.0]]]
    target_range: [[[-4.0, -1.0], [1.0, 4.0]]]
    samples_per_epoch: 16384
    batch_size: 16

model:
  _target_: icicl.models.telbanp.TELBANP
  encoder: ${telbanp_encoder}
  decoder: ${lbanp_decoder}
  likelihood: ${likelihood}

telbanp_encoder:
  _target_: icicl.models.telbanp.TELBANPEncoder
  nested_perceiver_encoder: ${teist_encoder}
  y_encoder: ${y_encoder}

teist_encoder:
  _target_: icicl.networks.tetransformer.NestedTEISetTransformerEncoder
  dim: ${params.dim_x}
  num_latents: ${params.num_latents}
  mhca_ctoq_layer: ${mhca_ctoq_layer}
  mhca_qtoc_layer: ${mhca_qtoc_layer}
  mhca_qtot_layer: ${mhca_qtot_layer}
  num_layers: ${params.num_layers}
  pseudo_token_initialiser: ${pseudo_token_initialiser}

mhca_ctoq_layer:
  _target_: icicl.networks.teattention_layers.MultiHeadCrossTEAttentionLayer
  embed_dim: ${params.embed_dim}
  num_heads: ${params.num_heads}
  head_dim: ${params.head_dim}
  kernel: ${mhca_ctoq_kernel}
  feedforward_dim: ${params.embed_dim}
  norm_first: ${params.norm_first}
  post_kernel: ${params.post_kernel}
  phi_t: ${mhca_ctoq_phi_t}
  qk_dim: ${params.qk_dim}

mhca_ctoq_kernel:
  _target_: icicl.networks.kernels.MLPKernel
  in_dim: ${eval:'${params.qk_dim} + ${params.dim_x}'}
  out_dim: ${params.num_heads}
  num_layers: 2
  width: ${params.embed_dim}

mhca_ctoq_phi_t:
  _target_: icicl.networks.mlp.MLP
  in_dim: ${params.num_heads}
  out_dim: ${params.dim_x}
  num_layers: 2
  width: ${params.embed_dim}

mhca_qtoc_layer:
  _target_: icicl.networks.teattention_layers.MultiHeadCrossTEAttentionLayer
  embed_dim: ${params.embed_dim}
  num_heads: ${params.num_heads}
  head_dim: ${params.head_dim}
  kernel: ${mhca_qtoc_kernel}
  feedforward_dim: ${params.embed_dim}
  norm_first: ${params.norm_first}
  post_kernel: ${params.post_kernel}
  phi_t: ${mhca_qtoc_phi_t}
  qk_dim: ${params.qk_dim}

mhca_qtoc_kernel:
  _target_: icicl.networks.kernels.MLPKernel
  in_dim: ${eval:'${params.qk_dim} + ${params.dim_x}'}
  out_dim: ${params.num_heads}
  num_layers: 2
  width: ${params.embed_dim}

mhca_qtoc_phi_t:
  _target_: icicl.networks.mlp.MLP
  in_dim: ${params.num_heads}
  out_dim: ${params.dim_x}
  num_layers: 2
  width: ${params.embed_dim}

mhca_qtot_layer:
  _target_: icicl.networks.teattention_layers.MultiHeadCrossTEAttentionLayer
  embed_dim: ${params.embed_dim}
  num_heads: ${params.num_heads}
  head_dim: ${params.head_dim}
  kernel: ${mhca_qtot_kernel}
  feedforward_dim: ${params.embed_dim}
  norm_first: ${params.norm_first}
  post_kernel: ${params.post_kernel}
  phi_t: ${mhca_qtot_phi_t}
  qk_dim: ${params.qk_dim}

mhca_qtot_kernel:
  _target_: icicl.networks.kernels.MLPKernel
  in_dim: ${eval:'${params.qk_dim} + ${params.dim_x}'}
  out_dim: ${params.num_heads}
  num_layers: 2
  width: ${params.embed_dim}

mhca_qtot_phi_t:
  _target_: icicl.networks.mlp.MLP
  in_dim: ${params.num_heads}
  out_dim: ${params.dim_x}
  num_layers: 2
  width: ${params.embed_dim}

pseudo_token_initialiser:
  _target_: icicl.networks.te_pseudo_initialisers.PseudoTokenInitialiser
  embed_dim: ${params.embed_dim}
  num_heads: ${params.num_heads}
  head_dim: ${params.head_dim}

y_encoder:
  _target_: icicl.networks.mlp.MLP
  in_dim: ${eval:'${params.dim_y} + 1'}
  out_dim: ${params.embed_dim}
  num_layers: 2
  width: ${params.embed_dim}

lbanp_decoder:
  _target_: icicl.models.lbanp.NestedLBANPDecoder
  z_decoder: ${z_decoder}

z_decoder:
  _target_: icicl.networks.mlp.MLP
  in_dim: ${params.embed_dim}
  out_dim: ${eval:'${params.dim_y} * 2'}
  num_layers: 2
  width: ${params.embed_dim}

likelihood:
  _target_: icicl.likelihoods.gaussian.HeteroscedasticNormalLikelihood

optimiser:
  _target_: torch.optim.AdamW
  _partial_: True
  lr: 5e-4

params:
  epochs: 200
  dim_x: 1
  dim_y: 1
  embed_dim: 32
  num_heads: 4
  head_dim: 8
  num_latents: 32
  num_layers: 5
  norm_first: True
  post_kernel: True
  qk_dim: ${params.num_heads}

misc:
  project: synthetic-1d-mixture-inputs
  name: TEIST-L${params.num_layers}-H${params.num_heads}-D${params.embed_dim}-M${params.num_latents}
  resume_from_checkpoint: null
  logging: True
  seed: 0
  plot_interval: 10
  gradient_clip_val: 0.5
