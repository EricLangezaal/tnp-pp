generators:
  train:
    _target_: icicl.data.on_off_grid.SyntheticOOTGGenerator
    off_grid_generator: ${generators.off_grid_generator_train}
    grid_range: [[-3.0, 3.0]]
    points_per_unit: ${params.points_per_unit}
    samples_per_epoch: 16000
    batch_size: 16
  val:
    _target_: icicl.data.on_off_grid.SyntheticOOTGGenerator
    off_grid_generator: ${generators.off_grid_generator_val}
    grid_range: [[-3.0, 3.0]]
    points_per_unit: ${params.points_per_unit}
    samples_per_epoch: 4096
    batch_size: 16
    
  off_grid_generator_train:
    _target_: icicl.data.gp.RandomScaleGPGenerator
    dim: ${params.dim}
    kernel_type: eq
    min_log10_lengthscale: -0.602
    max_log10_lengthscale: 0.602
    noise_std: 0.2
    out_dim: ${params.gp_output_dim}
    min_num_ctx: 1
    max_num_ctx: 64
    min_num_trg: 128
    max_num_trg: 128
    context_range: [[-2.0, 2.0]]
    target_range: [[-3.0, 3.0]]
    samples_per_epoch: ${generators.train.samples_per_epoch}
    batch_size: ${generators.train.batch_size}
  off_grid_generator_val:
    _target_: icicl.data.gp.RandomScaleGPGenerator
    dim: ${params.dim}
    kernel_type: eq
    min_log10_lengthscale: -0.602
    max_log10_lengthscale: 0.602
    noise_std: 0.2
    out_dim: ${params.gp_output_dim}
    min_num_ctx: 1
    max_num_ctx: 64
    min_num_trg: 128
    max_num_trg: 128
    context_range: [[-2.0, 2.0]]
    target_range: [[-3.0, 3.0]]
    samples_per_epoch: ${generators.val.samples_per_epoch}
    batch_size: ${generators.val.batch_size}

model:
  _target_: icicl.models.lbanp.LBANP
  encoder: ${lbanp_encoder}
  decoder: ${lbanp_decoder}
  likelihood: ${likelihood}

lbanp_encoder:
  _target_: icicl.models.lbanp.LBANPEncoder
  perceiver_encoder: ${perceiver_encoder}
  xy_encoder: ${xy_encoder}

# perceiver_encoder:
#   _target_: icicl.networks.transformer.PerceiverEncoder
#   num_latents: 16
#   mhsa_layer: ${mhsa_layer}
#   mhca_layer: ${mhca_ctoq_layer}
#   num_layers: 5

perceiver_encoder:
  _target_: icicl.networks.transformer.ISetTransformerEncoder
  num_latents: 8
  mhca_ctoq_layer: ${mhca_ctoq_layer}
  mhca_qtoc_layer: ${mhca_ctoq_layer}
  num_layers: 5

mhsa_layer:
  _target_: icicl.networks.attention_layers.MultiHeadSelfAttentionLayer
  embed_dim: ${params.embed_dim}
  num_heads: ${params.num_heads}
  head_dim: ${params.head_dim}
  feedforward_dim: ${params.embed_dim}

mhca_ctoq_layer:
  _target_: icicl.networks.attention_layers.MultiHeadCrossAttentionLayer
  embed_dim: ${params.embed_dim}
  num_heads: ${params.num_heads}
  head_dim: ${params.head_dim}
  feedforward_dim: ${params.embed_dim}

xy_encoder:
  _target_: icicl.networks.mlp.MLP
  in_dim: ${eval:'${params.dim} + 2'} # unmodified, since on and off the grid are just after each other
  out_dim: ${params.embed_dim}
  num_layers: 2
  width: ${params.embed_dim}

lbanp_decoder:
  _target_: icicl.models.lbanp.LBANPDecoder
  perceiver_decoder: ${perceiver_decoder}
  z_decoder: ${z_decoder}

perceiver_decoder:
  _target_: icicl.networks.transformer.PerceiverDecoder
  mhca_layer: ${mhca_qtot_layer}
  num_layers: 1

mhca_qtot_layer:
  _target_: icicl.networks.attention_layers.MultiHeadCrossAttentionLayer
  embed_dim: ${params.embed_dim}
  num_heads: ${params.num_heads}
  head_dim: ${params.head_dim}
  feedforward_dim: ${params.embed_dim}

z_decoder:
  _target_: icicl.networks.mlp.MLP
  in_dim: ${params.embed_dim}
  out_dim: 2
  num_layers: 2
  width: ${params.embed_dim}

likelihood:
  _target_: icicl.likelihoods.gaussian.HeteroscedasticNormalLikelihood

optimiser:
  _target_: torch.optim.Adam
  _partial_: True
  lr: 3e-4

params:
  dim: 1
  gp_output_dim: 1
  points_per_unit: 4
  epochs: 50
  embed_dim: 64
  num_heads: 8
  head_dim: 8


misc:
  project: tnp-on-off-grid
  name: lbanp_baseline
  resume_from_checkpoint: null