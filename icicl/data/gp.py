import random
from abc import ABC, abstractmethod
from typing import Callable, Optional, Tuple

import gpytorch
import torch
import torch.distributions as td

from .data import GroundTruthPredictor, ICSyntheticGenerator, SyntheticGenerator

KERNEL_TYPES = [
    "random",
    "eq",
    "matern12",
    "matern32",
    "matern52",
    "noisy_mixture",
    "weakly_periodic",
]


class GPGeneratorBase(ABC):
    def __init__(
        self,
        *,
        noise_std: float,
        **kwargs,
    ):
        super().__init__(**kwargs)

        self.noise_std = noise_std

    def sample_outputs(
        self,
        x: torch.Tensor,
        xic: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, GroundTruthPredictor, torch.Tensor]:
        """Sample context and target outputs, given the inputs `x`.

        Arguments:
            x: Tensor of shape (batch_size, num_ctx + num_trg, dim) containing
                the context and target inputs.
            xic: Optional[Tensor] of shape (batch_size, num_dc, num_dc_ctx, dim)
                containing the inputs of the in-context datasets.

        Returns:
            y: Tensor of shape (batch_size, num_ctx + num_trg, 1) containing
                the context and target outputs.
            yic: Optional[Tensor] of shape (batch_size, num_dc, num_dc_ctx, 1)
                containing the outputs of the in-context datasets.
        """

        # Set up GP kernel
        kernel = self.set_up_kernel().to(x)
        gt_pred = self.set_up_ground_truth_gp(kernel=kernel)

        # Set up covariance at input locations
        with torch.no_grad():
            kxx = kernel(x.to(torch.float64)).evaluate()

        kxx += torch.eye(kxx.shape[-1], dtype=torch.float64) * self.noise_std**2.0

        # Sample from GP with zero mean and covariance kxx.
        py = td.MultivariateNormal(
            loc=torch.zeros(kxx.shape[:-1], dtype=torch.float64), covariance_matrix=kxx
        )
        y = py.sample().unsqueeze(-1)

        if xic is not None:
            # Compute covariance at in-context input locations.
            with torch.no_grad():
                kxx = kernel(xic.to(torch.float64)).evaluate()

            kxx += torch.eye(kxx.shape[-1], dtype=torch.float64) * self.noise_std**2.0

            # Sample from GP with zero mean and covariance kxx.
            py = td.MultivariateNormal(
                loc=torch.zeros(kxx.shape[:-1], dtype=torch.float64),
                covariance_matrix=kxx,
            )
            yic = py.sample().unsqueeze(-1)

            return y.to(torch.float32), gt_pred, yic.to(torch.float32)

        return y.to(torch.float32), gt_pred, None

    @abstractmethod
    def set_up_kernel(self) -> gpytorch.kernels.Kernel:
        """Set up GP kernel.

        Returns:
            kernel: GP kernel.
        """

    def set_up_ground_truth_gp(self, kernel: Callable) -> GroundTruthPredictor:
        """Set up GP kernel.

        Arguments:
            seed: Random seed.

        Returns:
            seed: Random seed generated by splitting.
            kernel: GP kernel.
        """
        return GPGroundTruthPredictor(kernel=kernel, noise_std=self.noise_std)


class RandomScaleGPGeneratorBase(GPGeneratorBase):
    noisy_mixture_long_lengthscale: float = 1.0
    weakly_periodic_period: float = 3.0

    def __init__(
        self,
        *,
        kernel_type: str,
        min_log10_lengthscale: float,
        max_log10_lengthscale: float,
        **kwargs,
    ):
        super().__init__(**kwargs)

        self.kernel_type = kernel_type
        self.min_log10_lengthscale = torch.as_tensor(
            min_log10_lengthscale, dtype=torch.float64
        )
        self.max_log10_lengthscale = torch.as_tensor(
            max_log10_lengthscale, dtype=torch.float64
        )

        assert (
            self.kernel_type in KERNEL_TYPES
        ), f"kernel_type must be in {KERNEL_TYPES}, found {self.kernel_type=}."

    def set_up_kernel(self) -> gpytorch.kernels.Kernel:
        # Sample lengthscale
        log10_lengthscale = (
            torch.rand(()) * (self.max_log10_lengthscale - self.min_log10_lengthscale)
            + self.min_log10_lengthscale
        )
        lengthscale = 10.0**log10_lengthscale

        if self.kernel_type == "random":
            kernel_type = random.choice(KERNEL_TYPES[1:])
        else:
            kernel_type = self.kernel_type

        if kernel_type == "eq":
            kernel = gpytorch.kernels.RBFKernel()
            kernel.lengthscale = lengthscale

        elif kernel_type == "matern12":
            kernel = gpytorch.kernels.MaternKernel(nu=0.5)
            kernel.lengthscale = lengthscale

        elif kernel_type == "matern32":
            kernel = gpytorch.kernels.MaternKernel(nu=1.5)
            kernel.lengthscale = lengthscale

        elif kernel_type == "matern52":
            kernel = gpytorch.kernels.MaternKernel(nu=2.5)
            kernel.lengthscale = lengthscale

        elif kernel_type == "noisy_mixture":
            kernel1 = gpytorch.kernels.RBFKernel()
            kernel1.lengthscale = lengthscale
            kernel2 = gpytorch.kernels.RBFKernel()
            kernel2.lengthscale = self.noisy_mixture_long_lengthscale

            kernel = kernel1 + kernel2

        elif kernel_type == "weakly_periodic":
            kernel1 = gpytorch.kernels.RBFKernel()
            kernel1.lengthscale = lengthscale
            kernel2 = gpytorch.kernels.PeriodicKernel()
            kernel2.period_length = self.weakly_periodic_period
            kernel = kernel1 + kernel2

        else:
            raise ValueError("Unknown kernel type.")

        return kernel


class RandomScaleGPGenerator(RandomScaleGPGeneratorBase, SyntheticGenerator):
    pass


class ICRandomScaleGPGenerator(RandomScaleGPGeneratorBase, ICSyntheticGenerator):
    pass


class GPGroundTruthPredictor(GroundTruthPredictor):
    def __init__(self, kernel: Callable, noise_std: float):
        self.kernel = kernel
        self.noise_std = noise_std

    def __call__(
        self,
        xc: torch.Tensor,
        yc: torch.Tensor,
        xt: torch.Tensor,
        yt: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
        dtype = xc.dtype

        xc = xc.to(torch.float64)
        yc = yc.to(torch.float64)
        xt = xt.to(torch.float64)
        num_ctx = xc.shape[-2]

        x = torch.cat((xc, xt), dim=-2)
        with torch.no_grad():
            kxx = self.kernel(x).evaluate()

        kxx += torch.eye(x.shape[-2], dtype=torch.float64) * self.noise_std**2.0

        kcc = kxx[:, :num_ctx, :num_ctx]
        kct = kxx[:, :num_ctx, num_ctx:]
        ktc = kxx[:, num_ctx:, :num_ctx]
        ktt = kxx[:, num_ctx:, num_ctx:]

        mean = (ktc @ torch.linalg.solve(kcc, yc))[..., 0]
        cov = ktt - ktc @ torch.linalg.solve(kcc, kct)
        std = torch.diagonal(cov, dim1=-2, dim2=-1).sqrt()

        if yt is not None:
            yt = yt.to(torch.float64)
            gt_loglik = td.Normal(loc=mean, scale=std).log_prob(yt[..., 0])
            gt_loglik = gt_loglik.sum(-1)
            gt_loglik = gt_loglik.to(dtype)

        else:
            gt_loglik = None

        mean = mean.to(dtype)[:, :, None]
        std = std.to(dtype)[:, :, None]

        return mean, std, gt_loglik
