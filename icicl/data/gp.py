import math
import random
from abc import ABC, abstractmethod
from typing import Callable, List, Optional, Tuple, Union

import gpytorch
import torch
import torch.distributions as td

from icicl.networks.kernels import GibbsKernel

from .base import GroundTruthPredictor
from .synthetic import SyntheticGeneratorBimodalInput, SyntheticGeneratorUniformInput

KERNEL_TYPES = [
    "eq",
    "matern12",
    "matern32",
    "matern52",
    "noisy_mixture",
    "weakly_periodic",
    "periodic",
    "noisy_periodic_mixture",
    "gibbs",
    "gibbs_random_switch",
]


class GPGeneratorBase(ABC):
    def __init__(
        self,
        *,
        noise_std: float,
        **kwargs,
    ):
        super().__init__(**kwargs)

        self.noise_std = noise_std

    def sample_outputs(
        self,
        x: torch.Tensor,
        xic: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, GroundTruthPredictor, Optional[torch.Tensor]]:
        """Sample context and target outputs, given the inputs `x`.

        Arguments:
            x: Tensor of shape (batch_size, num_ctx + num_trg, dim) containing
                the context and target inputs.
            xic: Optional[Tensor] of shape (batch_size, num_dc, num_dc_ctx, dim)
                containing the inputs of the in-context datasets.

        Returns:
            y: Tensor of shape (batch_size, num_ctx + num_trg, 1) containing
                the context and target outputs.
            yic: Optional[Tensor] of shape (batch_size, num_dc, num_dc_ctx, 1)
                containing the outputs of the in-context datasets.
        """

        # Set up GP kernel
        kernel = self.set_up_kernel().to(x)
        gt_pred = self.set_up_ground_truth_gp(kernel=kernel)

        # Set up covariance at input locations
        with torch.no_grad():
            kxx = kernel(x.to(torch.float64)).evaluate()

        kxx += torch.eye(kxx.shape[-1], dtype=torch.float64) * self.noise_std**2.0

        # Sample from GP with zero mean and covariance kxx.
        py = td.MultivariateNormal(
            loc=torch.zeros(kxx.shape[:-1], dtype=torch.float64), covariance_matrix=kxx
        )
        y = py.sample().unsqueeze(-1)

        if xic is not None:
            # Compute covariance at in-context input locations.
            with torch.no_grad():
                kxx = kernel(xic.to(torch.float64)).evaluate()

            kxx += torch.eye(kxx.shape[-1], dtype=torch.float64) * self.noise_std**2.0

            # Sample from GP with zero mean and covariance kxx.
            py = td.MultivariateNormal(
                loc=torch.zeros(kxx.shape[:-1], dtype=torch.float64),
                covariance_matrix=kxx,
            )
            yic = py.sample().unsqueeze(-1)

            return y.to(torch.float32), gt_pred, yic.to(torch.float32)

        return y.to(torch.float32), gt_pred, None

    @abstractmethod
    def set_up_kernel(self) -> gpytorch.kernels.Kernel:
        """Set up GP kernel.

        Returns:
            kernel: GP kernel.
        """

    def set_up_ground_truth_gp(self, kernel: Callable) -> GroundTruthPredictor:
        """Set up GP kernel.

        Arguments:
            seed: Random seed.

        Returns:
            seed: Random seed generated by splitting.
            kernel: GP kernel.
        """
        return GPGroundTruthPredictor(kernel=kernel, noise_std=self.noise_std)


class RandomScaleGPGeneratorBase(GPGeneratorBase):
    noisy_mixture_long_lengthscale: float = 1.0
    weakly_periodic_period: float = 1.0

    def __init__(
        self,
        *,
        kernel_type: Union[List[str], str],
        min_log10_lengthscale: float,
        max_log10_lengthscale: float,
        **kwargs,
    ):
        super().__init__(**kwargs)

        self.kernel_type = kernel_type
        self.min_log10_lengthscale = torch.as_tensor(
            min_log10_lengthscale, dtype=torch.float64
        )
        self.max_log10_lengthscale = torch.as_tensor(
            max_log10_lengthscale, dtype=torch.float64
        )

    def set_up_kernel(self) -> gpytorch.kernels.Kernel:
        # Sample lengthscale
        log10_lengthscale = (
            torch.rand(()) * (self.max_log10_lengthscale - self.min_log10_lengthscale)
            + self.min_log10_lengthscale
        )
        lengthscale = 10.0**log10_lengthscale

        if isinstance(self.kernel_type, str):
            kernel_type = self.kernel_type
        else:
            kernel_type = random.choice(self.kernel_type)

        if kernel_type == "eq":
            kernel = gpytorch.kernels.RBFKernel()
            kernel.lengthscale = lengthscale

        elif kernel_type == "matern12":
            kernel = gpytorch.kernels.MaternKernel(nu=0.5)
            kernel.lengthscale = lengthscale

        elif kernel_type == "matern32":
            kernel = gpytorch.kernels.MaternKernel(nu=1.5)
            kernel.lengthscale = lengthscale

        elif kernel_type == "matern52":
            kernel = gpytorch.kernels.MaternKernel(nu=2.5)
            kernel.lengthscale = lengthscale

        elif kernel_type == "noisy_mixture":
            kernel1 = gpytorch.kernels.RBFKernel()
            kernel1.lengthscale = lengthscale
            kernel2 = gpytorch.kernels.RBFKernel()
            kernel2.lengthscale = self.noisy_mixture_long_lengthscale

            kernel = kernel1 + kernel2

        elif kernel_type == "weakly_periodic":
            kernel1 = gpytorch.kernels.RBFKernel()
            kernel1.lengthscale = lengthscale
            kernel2 = gpytorch.kernels.PeriodicKernel()
            kernel2.period_length = self.weakly_periodic_period
            kernel = kernel1 + kernel2

        elif kernel_type == "periodic":
            kernel = gpytorch.kernels.PeriodicKernel()
            kernel.period_length = lengthscale

        elif kernel_type == "noisy_periodic_mixture":
            kernel1 = gpytorch.kernels.PeriodicKernel()
            kernel1.period_length = lengthscale
            kernel2 = gpytorch.kernels.RBFKernel()
            kernel2.lengthscale = self.noisy_mixture_long_lengthscale
            kernel = kernel1 + kernel2

        elif kernel_type == "gibbs":
            lengthscale_fn = lambda x: torch.where(
                x[..., :1] < 0,
                torch.ones(*x[..., :1].shape).to(x) * 4.0,
                torch.ones(*x[..., :1].shape).to(x) * lengthscale,
            )

            kernel = GibbsKernel(lengthscale_fn=lengthscale_fn)

        elif kernel_type == "gibbs_random_switch":
            # x0 = torch.rand((1,)) * 6 - 3
            x0 = torch.as_tensor(random.choice([-1.0, 0.0, 1.0]))
            lengthscale_fn = lambda x: torch.where(
                x[..., :1] < x0.to(x),
                torch.ones(*x[..., :1].shape).to(x) * 4.0,
                torch.ones(*x[..., :1].shape).to(x) * lengthscale,
            )

            kernel = GibbsKernel(lengthscale_fn=lengthscale_fn)

        elif kernel_type == "gibbs_periodic":
            min_lengthscale = 0.5
            period_length = lengthscale
            offset = torch.rand((1,)) * 2 * math.pi
            lengthscale_fn = (
                lambda x: (
                    torch.sin(math.pi * x[..., :1] / period_length + offset.to(x))
                )
                ** 2
                + min_lengthscale
            )
            kernel = GibbsKernel(lengthscale_fn=lengthscale_fn)

        else:
            raise ValueError("Unknown kernel type.")

        return kernel


class RandomScaleGPGenerator(
    RandomScaleGPGeneratorBase, SyntheticGeneratorUniformInput
):
    pass


class RandomScaleGPGeneratorBimodalInput(
    RandomScaleGPGeneratorBase, SyntheticGeneratorBimodalInput
):
    pass


class GPGroundTruthPredictor(GroundTruthPredictor):
    def __init__(self, kernel: gpytorch.kernels.Kernel, noise_std: float):
        self.kernel = kernel
        self.noise_std = noise_std

    def __call__(
        self,
        xc: torch.Tensor,
        yc: torch.Tensor,
        xt: torch.Tensor,
        yt: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
        dtype = xc.dtype

        xc = xc.to(torch.float64)
        yc = yc.to(torch.float64)
        xt = xt.to(torch.float64)
        num_ctx = xc.shape[-2]

        x = torch.cat((xc, xt), dim=-2)
        with torch.no_grad():
            kxx = self.kernel.to(x.device)(x).evaluate()

        kxx += (
            torch.eye(x.shape[-2], dtype=torch.float64).to(x.device)
            * self.noise_std**2.0
        )

        kcc = kxx[:, :num_ctx, :num_ctx]
        kct = kxx[:, :num_ctx, num_ctx:]
        ktc = kxx[:, num_ctx:, :num_ctx]
        ktt = kxx[:, num_ctx:, num_ctx:]

        mean = (ktc @ torch.linalg.solve(kcc, yc))[  # pylint: disable=not-callable
            ..., 0
        ]
        cov = ktt - ktc @ torch.linalg.solve(kcc, kct)  # pylint: disable=not-callable
        std = torch.diagonal(cov, dim1=-2, dim2=-1).sqrt()

        if yt is not None:
            yt = yt.to(torch.float64)
            gt_loglik = td.Normal(loc=mean, scale=std).log_prob(yt[..., 0])
            gt_loglik = gt_loglik.sum(-1)
            gt_loglik = gt_loglik.to(dtype)

        else:
            gt_loglik = None

        mean = mean.to(dtype)[:, :, None]
        std = std.to(dtype)[:, :, None]

        return mean, std, gt_loglik
