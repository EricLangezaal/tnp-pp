{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93e940-4132-4cdb-a736-287c342f9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d861c-3ef8-4ed7-be05-cd00be284585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import neuralprocesses as nps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d2e22-8e65-42f6-80a9-b78527e06347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralprocesses.util import register_model\n",
    "from neuralprocesses.architectures.convgnp import _convgnp_assert_form_contexts\n",
    "from neuralprocesses.architectures.util import construct_likelihood, parse_transform\n",
    "\n",
    "\n",
    "@register_model\n",
    "def construct_gnp(\n",
    "    dim_x=1,\n",
    "    dim_y=1,\n",
    "    dim_yc=None,\n",
    "    dim_yt=None,\n",
    "    dim_embedding=256,\n",
    "    dim_dataset_embedding=32,\n",
    "    attention=False,\n",
    "    attention_num_heads=8,\n",
    "    num_enc_layers=3,\n",
    "    enc_same=False,\n",
    "    num_dec_layers=6,\n",
    "    width=512,\n",
    "    likelihood=\"lowrank\",\n",
    "    num_basis_functions=512,\n",
    "    dim_lv=0,\n",
    "    lv_likelihood=\"het\",\n",
    "    transform=None,\n",
    "    dtype=None,\n",
    "    nps=nps,\n",
    "):\n",
    "    \"\"\"A Gaussian Neural Process.\n",
    "\n",
    "    Args:\n",
    "        dim_x (int, optional): Dimensionality of the inputs. Defaults to 1.\n",
    "        dim_y (int, optional): Dimensionality of the outputs. Defaults to 1.\n",
    "        dim_yc (int or tuple[int], optional): Dimensionality of the outputs of the\n",
    "            context set. You should set this if the dimensionality of the outputs\n",
    "            of the context set is not equal to the dimensionality of the outputs\n",
    "            of the target set. You should also set this if you want to use multiple\n",
    "            context sets. In that case, set this equal to a tuple of integers\n",
    "            indicating the respective output dimensionalities.\n",
    "        dim_yt (int, optional): Dimensionality of the outputs of the target set. You\n",
    "            should set this if the dimensionality of the outputs of the target set is\n",
    "            not equal to the dimensionality of the outputs of the context set.\n",
    "        dim_embedding (int, optional): Dimensionality of the embedding. Defaults to 128.\n",
    "        attention (bool, optional): Use attention for the deterministic encoder.\n",
    "            Defaults to `False`.\n",
    "        attention_num_heads (int, optional): Number of heads. Defaults to `8`.\n",
    "        num_enc_layers (int, optional): Number of layers in the encoder. Defaults to 3.\n",
    "        enc_same (bool, optional): Use the same encoder for all context sets. This\n",
    "            only works if all context sets have the same dimensionality. Defaults to\n",
    "            `False`.\n",
    "        num_dec_layers (int, optional): Number of layers in the decoder. Defaults to 6.\n",
    "        width (int, optional): Widths of all intermediate MLPs. Defaults to 512.\n",
    "        likelihood (str, optional): Likelihood. Must be one of `\"het\"` or `\"lowrank\"`.\n",
    "            Defaults to `\"lowrank\"`.\n",
    "        num_basis_functions (int, optional): Number of basis functions for the\n",
    "            low-rank likelihood. Defaults to 512.\n",
    "        dim_lv (int, optional): Dimensionality of the latent variable. Defaults to 0.\n",
    "        lv_likelihood (str, optional): Likelihood of the latent variable. Must be one of\n",
    "            `\"het\"`, `\"dense\"`, or `\"spikes-beta\"`. Defaults to `\"het\"`.\n",
    "        transform (str or tuple[float, float]): Bijection applied to the\n",
    "            output of the model. This can help deal with positive of bounded data.\n",
    "            Must be either `\"positive\"`, `\"exp\"`, `\"softplus\"`, or\n",
    "            `\"softplus_of_square\"` for positive data or `(lower, upper)` for data in\n",
    "            this open interval.\n",
    "        dtype (dtype, optional): Data type.\n",
    "\n",
    "    Returns:\n",
    "        :class:`.model.Model`: GNP model.\n",
    "    \"\"\"\n",
    "    # Make sure that `dim_yc` is initialised and a tuple.\n",
    "    dim_yc = convert(dim_yc or dim_y, tuple)\n",
    "    # Make sure that `dim_yt` is initialised.\n",
    "    dim_yt = dim_yt or dim_y\n",
    "\n",
    "    # Check if `enc_same` can be used.\n",
    "    if enc_same and any(dim_yci != dim_yc[0] for dim_yci in dim_yc[1:]):\n",
    "        raise ValueError(\n",
    "            \"Can only use the same encoder for all context sets if the context sets \"\n",
    "            \"are of the same dimensionality, but they are not.\"\n",
    "        )\n",
    "\n",
    "    mlp_out_channels, selector, likelihood = construct_likelihood(\n",
    "        nps,\n",
    "        spec=likelihood,\n",
    "        dim_y=dim_yt,\n",
    "        num_basis_functions=num_basis_functions,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    # Construct the deterministic encoder.\n",
    "    if attention:\n",
    "\n",
    "        def construct_attention(dim_yci):\n",
    "            return nps.Attention(\n",
    "                dim_x=dim_x,\n",
    "                dim_y=dim_yci,\n",
    "                dim_embedding=dim_embedding,\n",
    "                num_heads=attention_num_heads,\n",
    "                num_enc_layers=num_enc_layers,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "\n",
    "        if enc_same:\n",
    "            block = construct_attention(dim_yc[0])\n",
    "\n",
    "        det_encoder = nps.Parallel(\n",
    "            *(\n",
    "                nps.Chain(\n",
    "                    nps.RepeatForAggregateInputs(\n",
    "                        block if enc_same else construct_attention(dim_yci)\n",
    "                    ),\n",
    "                    nps.DeterministicLikelihood(),\n",
    "                )\n",
    "                for dim_yci in dim_yc\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        def construct_mlp(dim_yci):\n",
    "            return nps.MLP(\n",
    "                in_dim=dim_x + dim_yci,\n",
    "                out_dim=dim_embedding,\n",
    "                num_layers=num_enc_layers,\n",
    "                width=width,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "\n",
    "        if enc_same:\n",
    "            block = construct_mlp(dim_yc[0])\n",
    "\n",
    "        det_encoder = nps.Parallel(\n",
    "            *(\n",
    "                nps.Chain(\n",
    "                    nps.DeepSet(block if enc_same else construct_mlp(dim_yci)),\n",
    "                    nps.DeterministicLikelihood(),\n",
    "                )\n",
    "                for dim_yci in dim_yc\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Possibly construct the stochastic encoder.\n",
    "    if dim_lv > 0:\n",
    "        lv_mlp_out_channels, _, lv_likelihood = construct_likelihood(\n",
    "            nps,\n",
    "            spec=lv_likelihood,\n",
    "            dim_y=dim_lv,\n",
    "            num_basis_functions=num_basis_functions,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "\n",
    "        def construct_mlp(dim_yci):\n",
    "            return nps.MLP(\n",
    "                in_dim=dim_x + dim_yci,\n",
    "                out_dim=dim_embedding,\n",
    "                num_layers=num_enc_layers,\n",
    "                width=width,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "\n",
    "        if enc_same:\n",
    "            block = construct_mlp(dim_yc[0])\n",
    "\n",
    "        lv_encoder = nps.Chain(\n",
    "            nps.Parallel(\n",
    "                *(\n",
    "                    nps.DeepSet(block if enc_same else construct_mlp(dim_yci))\n",
    "                    for dim_yci in dim_yc\n",
    "                ),\n",
    "            ),\n",
    "            nps.Concatenate(),\n",
    "            nps.MLP(\n",
    "                in_dim=dim_embedding * len(dim_yc),\n",
    "                out_dim=lv_mlp_out_channels,\n",
    "                num_layers=num_enc_layers,\n",
    "                # The capacity of this MLP should increase with the number of outputs,\n",
    "                # but multiplying by `len(dim_yc)` is too aggressive.\n",
    "                width=width * min(len(dim_yc), 2),\n",
    "                dtype=dtype,\n",
    "            ),\n",
    "            lv_likelihood,\n",
    "        )\n",
    "\n",
    "    encoder = nps.Chain(\n",
    "        # We need to explicitly copy, because there will be multiple context sets in\n",
    "        # parallel, which will otherwise dispatch to the wrong method.\n",
    "        _convgnp_assert_form_contexts(nps, dim_yc),\n",
    "        nps.Copy(2 + (dim_lv > 0)),\n",
    "        nps.Parallel(\n",
    "            nps.Chain(\n",
    "                nps.RepeatForAggregateInputs(\n",
    "                    nps.InputsCoder(),\n",
    "                ),\n",
    "                nps.DeterministicLikelihood(),\n",
    "            ),\n",
    "            det_encoder,\n",
    "            *((lv_encoder,) if dim_lv > 0 else ()),\n",
    "        ),\n",
    "    )\n",
    "    decoder = nps.Chain(\n",
    "        nps.Concatenate(),\n",
    "        nps.RepeatForAggregateInputs(\n",
    "            nps.Chain(\n",
    "                nps.MLP(\n",
    "                    in_dim=dim_x + dim_embedding * len(dim_yc) + dim_lv,\n",
    "                    out_dim=mlp_out_channels,\n",
    "                    num_layers=num_dec_layers,\n",
    "                    # The capacity of this MLP should increase with the number of\n",
    "                    # outputs, but multiplying by `len(dim_yc)` is too aggressive.\n",
    "                    width=width * min(len(dim_yc), 2),\n",
    "                    dtype=dtype,\n",
    "                ),\n",
    "                selector,  # Select the right target output.\n",
    "            )\n",
    "        ),\n",
    "        likelihood,\n",
    "        parse_transform(nps, transform=transform),\n",
    "    )\n",
    "    return nps.Model(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427072e9-bb06-4f95-a69a-08e659f622d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
